# Real-Time Sentiment Analysis System

A production-ready, real-time sentiment analysis pipeline using **Lambda Architecture** for processing Reddit data at scale.

![Architecture](docs/architecture.png)

## ğŸ—ï¸ Architecture Overview

This system implements the **Lambda Architecture** pattern with three layers:

### 1. **Batch Layer** (Spark Batch)
- Processes historical data stored in HDFS
- Computes comprehensive aggregations hourly
- Generates batch views for accurate analytics
- Handles data reprocessing and backfilling

### 2. **Speed Layer** (Spark Streaming)
- Processes real-time data from Kafka
- Provides low-latency sentiment analysis
- Stores results in Redis for fast access
- Enables real-time alerting

### 3. **Serving Layer** (FastAPI)
- Combines batch and real-time views
- Exposes REST API for querying
- Handles data fusion and presentation
- Provides alerting and monitoring endpoints

## ğŸš€ Features

- **Real-time Reddit scraping** with PRAW
- **BERT-based sentiment analysis** (multilingual support)
- **Keyword extraction** and entity recognition
- **Sentiment alerts** with configurable thresholds
- **Topic-based partitioning** in HDFS
- **Trend analysis** and anomaly detection
- **Subreddit-level breakdown**
- **Topic comparison** functionality
- **Prometheus metrics** and Grafana dashboards
- **Fully containerized** with Docker Compose

## ğŸ“‹ Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- 8GB+ RAM recommended
- Reddit API credentials

## ğŸ”§ Quick Start

### 1. Clone the repository

```bash
git clone <repository-url>
cd Real-time-senti
```

### 2. Configure environment

Create a `.env` file (or copy from `.env.example`):

```bash
# Reddit API Credentials
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=YourApp/1.0 by u/YourUsername

# Other configurations are pre-set with defaults
```

### 3. Start the system

```bash
# Make scripts executable
chmod +x scripts/*.sh

# Start all services
./scripts/start.sh start
```

### 4. Verify the setup

```bash
# Check service health
./scripts/health_check.sh
```

## ğŸŒ Service Endpoints

| Service | URL | Description |
|---------|-----|-------------|
| **API** | http://localhost:8000 | Main REST API |
| **API Docs** | http://localhost:8000/docs | Swagger documentation |
| **Kafka UI** | http://localhost:8080 | Kafka management |
| **Spark UI** | http://localhost:8081 | Spark cluster dashboard |
| **HDFS UI** | http://localhost:9870 | Hadoop filesystem |
| **Grafana** | http://localhost:3000 | Monitoring dashboards |
| **Prometheus** | http://localhost:9090 | Metrics collection |

## ğŸ“¡ API Usage

### Get Combined Sentiment View

```bash
curl http://localhost:8000/api/v1/sentiment/technology
```

### Get Real-time Sentiment

```bash
curl http://localhost:8000/api/v1/sentiment/technology/realtime
```

### Get Sentiment Trends

```bash
curl http://localhost:8000/api/v1/sentiment/technology/trends?hours=24
```

### Get Top Keywords

```bash
curl http://localhost:8000/api/v1/sentiment/technology/keywords?limit=20
```

### Compare Topics

```bash
curl "http://localhost:8000/api/v1/compare?topics=technology,finance"
```

### Create New Topic

```bash
curl -X POST http://localhost:8000/api/v1/topics \
  -H "Content-Type: application/json" \
  -d '{
    "name": "crypto",
    "subreddits": ["cryptocurrency", "bitcoin", "ethereum"],
    "keywords": ["btc", "eth", "blockchain"],
    "active": true
  }'
```

### Get Alerts

```bash
curl http://localhost:8000/api/v1/alerts?unacknowledged_only=true
```

## ğŸ“ Project Structure

```
Real-time-senti/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py          # Configuration management
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ api/                  # API Dockerfile
â”‚   â”œâ”€â”€ grafana/              # Grafana provisioning
â”‚   â”œâ”€â”€ mongodb/              # MongoDB initialization
â”‚   â”œâ”€â”€ prometheus/           # Prometheus config
â”‚   â”œâ”€â”€ scraper/              # Scraper Dockerfile
â”‚   â”œâ”€â”€ spark/                # Spark master Dockerfile
â”‚   â”œâ”€â”€ spark-batch/          # Batch processor Dockerfile
â”‚   â””â”€â”€ spark-streaming/      # Streaming processor Dockerfile
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ api.txt               # API dependencies
â”‚   â”œâ”€â”€ scraper.txt           # Scraper dependencies
â”‚   â”œâ”€â”€ spark-batch.txt       # Batch dependencies
â”‚   â””â”€â”€ spark-streaming.txt   # Streaming dependencies
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_topics.sh      # Kafka topic creation
â”‚   â”œâ”€â”€ health_check.sh       # System health check
â”‚   â”œâ”€â”€ setup_hdfs.sh         # HDFS directory setup
â”‚   â””â”€â”€ start.sh              # Main startup script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py           # FastAPI serving layer
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â””â”€â”€ spark_batch.py    # Spark batch processing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ data_models.py    # Pydantic models
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py  # BERT sentiment analysis
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â””â”€â”€ reddit_scraper.py # Reddit data scraper
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â””â”€â”€ spark_streaming.py # Spark streaming
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logging_config.py # Logging configuration
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml        # Docker orchestration
â””â”€â”€ README.md
```

## ğŸ”„ Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Reddit    â”‚â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â–¶â”‚ Spark Streaming  â”‚
â”‚   Scraper   â”‚    â”‚         â”‚    â”‚  (Speed Layer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚    Redis     â”‚
                                    â”‚  (Real-time) â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                                               â”‚
                   â–¼                                               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     HDFS     â”‚                                â”‚   MongoDB    â”‚
            â”‚  (Raw Data)  â”‚                                â”‚  (Results)   â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                                               â”‚
                   â–¼                                               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
            â”‚ Spark Batch  â”‚                                       â”‚
            â”‚(Batch Layer) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   FastAPI    â”‚â—€â”€â”€â”€ Serving Layer (combines views)
            â”‚  (REST API)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `REDDIT_CLIENT_ID` | Reddit API client ID | Required |
| `REDDIT_CLIENT_SECRET` | Reddit API secret | Required |
| `REDDIT_USER_AGENT` | Reddit API user agent | Required |
| `SCRAPE_INTERVAL_SECONDS` | Scraping interval | 60 |
| `BATCH_INTERVAL_HOURS` | Batch processing interval | 1 |
| `SENTIMENT_ALERT_THRESHOLD` | Alert threshold | -0.5 |
| `LOG_LEVEL` | Logging level | INFO |

### Adding New Topics

Topics can be managed via the API:

```bash
# Add a new topic
curl -X POST http://localhost:8000/api/v1/topics \
  -H "Content-Type: application/json" \
  -d '{
    "name": "ai",
    "subreddits": ["artificial", "MachineLearning", "deeplearning"],
    "keywords": ["GPT", "LLM", "neural network"],
    "active": true
  }'

# Deactivate a topic
curl -X PUT http://localhost:8000/api/v1/topics/ai \
  -H "Content-Type: application/json" \
  -d '{"active": false}'
```

## ğŸ“Š Monitoring

### Grafana Dashboards

Access Grafana at http://localhost:3000 (admin/sentiment_grafana_2024)

Pre-configured dashboards:
- System Overview
- Sentiment Trends
- Kafka Metrics
- API Performance

### Prometheus Metrics

Access at http://localhost:9090

Available metrics:
- `api_requests_total` - Total API requests
- `api_request_latency_seconds` - Request latency
- Kafka consumer lag
- Spark job metrics

## ğŸ› ï¸ Development

### Running Locally (without Docker)

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements/api.txt

# Run individual services
python -m src.scraper.reddit_scraper
python -m src.streaming.spark_streaming
python -m src.batch.spark_batch
python -m src.api.main
```

### Running Tests

```bash
# Run unit tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“ Commands Reference

```bash
# Start all services
./scripts/start.sh start

# Stop all services
./scripts/start.sh stop

# Restart services
./scripts/start.sh restart

# View logs
./scripts/start.sh logs                    # All services
./scripts/start.sh logs reddit-scraper     # Specific service

# Check status
./scripts/start.sh status

# Clean up (removes all data)
./scripts/start.sh clean

# Rebuild images
./scripts/start.sh build
```

## ğŸ” Troubleshooting

### Common Issues

1. **Kafka connection errors**
   - Wait for Kafka to fully start (check Kafka UI)
   - Run `./scripts/create_topics.sh`

2. **HDFS permission errors**
   - Run `./scripts/setup_hdfs.sh`

3. **Out of memory errors**
   - Increase Docker memory allocation
   - Reduce `spark.executor.memory` in config

4. **Reddit rate limiting**
   - Increase `SCRAPE_INTERVAL_SECONDS`
   - Reduce `posts_per_subreddit`

### Viewing Logs

```bash
# All services
docker compose logs -f

# Specific service
docker compose logs -f reddit-scraper
docker compose logs -f spark-streaming
docker compose logs -f serving-api
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“§ Support

For issues and questions, please open a GitHub issue.
